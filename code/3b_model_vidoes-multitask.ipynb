{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daaa341c-0948-46ae-8454-8e47c62cf06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db27084b-c3c0-4e7a-bc00-dab2262def34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from torch.utils.data import DataLoader\n",
    "import timm\n",
    "from timm import optim, scheduler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics as skmet\n",
    "from jupyterplot import ProgressPlot\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "\n",
    "import copy\n",
    "\n",
    "import transforms as my_transforms\n",
    "from dataset import VideoData\n",
    "from models_multitask import FrameClassifier, VideoClassifier_PI_PI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d90ba8-0f7e-4028-8c0a-f021b9510880",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937afd73-1090-461b-b77a-0a083625eece",
   "metadata": {},
   "source": [
    "### Import Frame Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af05b89-c3b2-4d84-8f4a-ab907f26fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_folder = '/zfs/wficai/pda/model_run_artifacts/20220818_multitask_224x224'\n",
    "\n",
    "with open(artifact_folder + '/config.json', 'r') as f: \n",
    "    cfg = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d658c33-a1f0-4e11-81ca-a8d08b5b14dc",
   "metadata": {},
   "source": [
    "### Set Video-level settings\n",
    "Some will override frame model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b76cd31-d40e-4e38-8531-b5b60d4757c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_video = dict(\n",
    "    bs_train = 6,  # batch size for training\n",
    "    bs_test = 6,  # batch size for testing\n",
    "    num_workers = 30,  # number of parallel data loading workers\n",
    "    device = 'cuda:0',\n",
    "    num_epochs=20,\n",
    "    lr = 0.001,\n",
    "    lr_unfrozen = 0.0001,\n",
    "    lr_gamma = 0.92,\n",
    "    time_downsample_factor = 8,\n",
    "    time_downsample_method = 'random',\n",
    "    dropout = 0.3,\n",
    "    weight_decay = 0.001,\n",
    "    pretrained=True,\n",
    "    unfreeze_after_n=3,\n",
    "    video_transforms = dict(\n",
    "        train = 'train',\n",
    "        test = 'test'\n",
    "    )\n",
    ")\n",
    "\n",
    "cfg.update(cfg_video)\n",
    "\n",
    "with open(artifact_folder + '/config_video.json', 'w') as f:\n",
    "    json.dump(cfg, f, indent=4)\n",
    "\n",
    "# put all config variables in scope to avoid the need to laboriously index cfg\n",
    "for k, v in cfg.items():\n",
    "    v = f\"'{v}'\" if type(v)==str else v\n",
    "    exec(f\"{k}={v}\")\n",
    "    \n",
    "del cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40792ac2-3896-4333-be17-a0c151423d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f5747c8-11f3-4edc-97e7-c4b6bdf3d563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((152630, 14), (43956, 14))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to use the same train/test split as was used in the frame model to avoid data leakage\n",
    "df_train = pd.read_csv(f'{artifact_folder}/{out_paths[\"train\"]}')\n",
    "df_test = pd.read_csv(f'{artifact_folder}/{out_paths[\"test\"]}')\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab5c0db-7213-43dd-b85f-53ad4596d8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All disjoint checks passed\n"
     ]
    }
   ],
   "source": [
    "# ensure that patients are disjoint\n",
    "train_patient = set(df_train.patient_id)\n",
    "test_patient = set(df_test.patient_id)\n",
    "assert train_patient.isdisjoint(test_patient), 'Set of train patients and set of test patients are not disjoint!'\n",
    "\n",
    "# ensure that studies are disjoint\n",
    "train_study = set(df_train.study + df_train.patient_type)\n",
    "test_study = set(df_test.study + df_test.patient_type)\n",
    "assert train_study.isdisjoint(test_study), 'Set of train studies and set of test studies are not disjoint!'\n",
    "\n",
    "# ensure that videos are disjoint\n",
    "train_vids = set(df_train.external_id + df_train.patient_type)\n",
    "test_vids = set(df_test.external_id + df_test.patient_type)\n",
    "assert train_vids.isdisjoint(test_vids), 'Set of train videos and set of test videos are not disjoint!'\n",
    "\n",
    "# ensure that frames are disjoint\n",
    "train_frames = set(df_train.png_path)\n",
    "test_frames = set(df_test.png_path)\n",
    "assert train_frames.isdisjoint(test_frames), 'Set of train frames and set of test frames are not disjoint!'\n",
    "\n",
    "print(\"All disjoint checks passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c46042a3-6018-4497-ae4a-a40317232777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Compose(\n",
       "     RandomEqualize(p=0.5)\n",
       "     RandAugment(num_ops=2, magnitude=9, num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n",
       "     DownsampleTime()\n",
       "     ConvertImageDtype()\n",
       "     UpsamplingBilinear2d(size=246, mode=bilinear)\n",
       "     CenterCrop(size=(224, 224))\n",
       "     RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.3, 3.3), value=0, inplace=False)\n",
       "     Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
       "     RandomHorizontalFlip(p=0.5)\n",
       "     RandomRotation(degrees=[-45.0, 45.0], interpolation=nearest, expand=False, fill=0)\n",
       "     RandomInvert(p=0.5)\n",
       " ),\n",
       " Compose(\n",
       "     ConvertImageDtype()\n",
       "     UpsamplingBilinear2d(size=246, mode=bilinear)\n",
       "     CenterCrop(size=(224, 224))\n",
       "     Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
       " ))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfms = my_transforms.VideoTransforms(res, time_downsample_factor)\n",
    "tfms_train = tfms.get_transforms(transforms['train'])\n",
    "tfms_test = tfms.get_transforms(transforms['test'])\n",
    "tfms_train, tfms_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2df49f61-e396-46e1-9af9-9fa212099aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_dataloader, loss_function, device):\n",
    "    model.train()\n",
    "\n",
    "    num_steps_per_epoch = len(train_dataloader)\n",
    "\n",
    "    losses = []\n",
    "    for ix, batch in enumerate(train_dataloader):\n",
    "        inputs = batch['video'].to(device)\n",
    "        num_frames = batch['num_frames']\n",
    "        targets = {k: batch[k].to(device).type(torch.float32) for k in ('trg_type', 'trg_view', 'trg_mode')}\n",
    "        outputs, _ = model(inputs, num_frames)\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        losses.append(loss.detach().item())\n",
    "        print(f\"\\tBatch {ix+1} of {num_steps_per_epoch}. Loss={loss.detach().item():0.3f}\", end='\\r')\n",
    "    \n",
    "    print(' '*100, end='\\r')\n",
    "        \n",
    "    return np.mean(losses)\n",
    "            \n",
    "            \n",
    "def evaluate(model, test_dataloader, loss_function, device):\n",
    "    model.eval()\n",
    "\n",
    "    num_steps_per_epoch = len(test_dataloader)\n",
    "\n",
    "    patient_ls = []\n",
    "    target_ls = []\n",
    "    output_ls = []\n",
    "    losses = []\n",
    "    for ix, batch in enumerate(test_dataloader):\n",
    "        inputs = batch['video'].to(device)\n",
    "        num_frames = batch['num_frames']\n",
    "        targets = {k: batch[k].cpu().type(torch.float32).numpy() for k in ('trg_type', 'trg_view', 'trg_mode')}\n",
    "        target_ls.append(targets)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs, _ = model(inputs, num_frames)\n",
    "            outputs = {k: v.cpu().numpy() for k, v in outputs.items()}\n",
    "            output_ls.append(outputs)\n",
    "            loss = {k: v.detach().item() for k, v in loss_function(outputs, targets).items()}\n",
    "            \n",
    "        losses.append(loss)\n",
    "        \n",
    "    # metrics = compute_metrics(np.concatenate(target_ls), np.concatenate(output_ls))\n",
    "    return np.mean(losses), None #metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d0192d3-c53f-4e8e-bf7b-656a2270813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumanth todo: check out 3a_model_frames-multitask 'compute_metrics' function\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    mets = dict()\n",
    "    \n",
    "    y_pred_cls = (y_pred>0.5).astype(int)\n",
    "    \n",
    "    mets['roc_auc'] = skmet.roc_auc_score(y_true, y_pred)\n",
    "    mets['average_precision'] = skmet.average_precision_score(y_true, y_pred)\n",
    "    mets['accuracy'] = skmet.accuracy_score(y_true, y_pred_cls)\n",
    "    mets['sensitivity'] = skmet.recall_score(y_true, y_pred_cls)\n",
    "    mets['specificity'] = skmet.recall_score(y_true, y_pred_cls, pos_label=0)\n",
    "    \n",
    "    return mets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b176f57a-da3f-4f8e-8ba2-b348e8b9f65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 1692\n",
      "Test data size: 457\n"
     ]
    }
   ],
   "source": [
    "# create datasets\n",
    "d_train = VideoData(df_train, transforms = tfms_train, mode_filter = mode_filter, view_filter = view_filter)\n",
    "dl_train = DataLoader(d_train, batch_size=bs_train, num_workers=num_workers, shuffle=True, collate_fn=VideoData.collate, pin_memory=True)\n",
    "\n",
    "d_test = VideoData(df_test, transforms = tfms_test, mode_filter = mode_filter, view_filter = view_filter)\n",
    "dl_test = DataLoader(d_test, batch_size=bs_test, num_workers=num_workers, collate_fn=VideoData.collate, pin_memory=True)\n",
    "\n",
    "print(\"Train data size:\", len(d_train))\n",
    "print(\"Test data size:\", len(d_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3885b994-6634-4262-8bb4-a592d4336840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([86, 3, 224, 224])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch = next(iter(dl_train))\n",
    "test_batch['video'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25d3bce1-b636-42bc-b13a-21596408cb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 10, 19, 18, 13, 14]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch['num_frames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "494f4601-285c-44b4-a5b9-5b7dc248e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "del FrameClassifier, VideoClassifier_PI_PI\n",
    "from models_multitask import FrameClassifier, VideoClassifier_PI_PI\n",
    "\n",
    "encoder = timm.create_model(model, pretrained=pretrained, num_classes=1, in_chans=3, drop_rate=dropout)\n",
    "clf_frames = FrameClassifier(encoder, encoder_frozen=True).to(device)\n",
    "\n",
    "# load pretrained weights for frame classifier\n",
    "clf_frames.load_state_dict(torch.load(f\"{artifact_folder}/model_checkpoint.ckpt\"))\n",
    "\n",
    "loss_func = FrameClassifier.multi_task_loss\n",
    "\n",
    "# create video model\n",
    "m = VideoClassifier_PI_PI(clf_frames, encoder_frozen=True, frame_classifier_frozen=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c01dbf6-7418-4de0-a2f9-b556a4afc562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': tensor([[ 0.4764],\n",
       "         [-0.2844],\n",
       "         [-0.0137],\n",
       "         [ 0.2134],\n",
       "         [ 0.5181],\n",
       "         [ 0.2379]], device='cuda:0'),\n",
       " 'mode': tensor([[ 1.7057, -1.9115, -1.9183],\n",
       "         [ 2.6653, -0.3951, -4.6629],\n",
       "         [-4.1133,  0.0144,  1.9066],\n",
       "         [-1.0395, -3.2064,  2.0566],\n",
       "         [ 1.6044, -0.8762, -2.7260],\n",
       "         [ 0.2205, -0.6102, -1.7754]], device='cuda:0'),\n",
       " 'view': tensor([[ 0.5037, -0.3100, -1.2585],\n",
       "         [ 1.2725, -0.9084, -1.9620],\n",
       "         [ 0.2403, -0.5814, -0.9293],\n",
       "         [ 1.0006, -0.7483, -1.2813],\n",
       "         [ 0.4912, -0.7867, -1.2312],\n",
       "         [ 0.4742, -0.8788, -0.9785]], device='cuda:0')}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on test batch\n",
    "with torch.no_grad():\n",
    "    y, attn = m(test_batch['video'].to(device), test_batch['num_frames'])\n",
    "# y, test_batch['trg_type'], attn.shape\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bbe9e840-81b2-4b78-acb5-f26eb464a046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': tensor([[ 0.5183],\n",
       "         [-0.1875],\n",
       "         [-0.0223],\n",
       "         [ 0.2584],\n",
       "         [ 0.5277],\n",
       "         [ 0.2175]], device='cuda:0'),\n",
       " 'mode': tensor([[ 1.7078, -1.9479, -1.8719],\n",
       "         [ 2.6794, -0.4279, -4.6418],\n",
       "         [-4.1952,  0.0840,  1.9263],\n",
       "         [-1.0847, -3.1737,  2.0701],\n",
       "         [ 1.5608, -0.9756, -2.6079],\n",
       "         [ 0.1716, -0.5530, -1.7750]], device='cuda:0'),\n",
       " 'view': tensor([[ 0.4843, -0.3117, -1.2315],\n",
       "         [ 1.2347, -0.9009, -1.8831],\n",
       "         [ 0.2223, -0.5681, -0.9079],\n",
       "         [ 1.0061, -0.7631, -1.2782],\n",
       "         [ 0.3817, -0.7295, -1.1251],\n",
       "         [ 0.4934, -0.8746, -1.0141]], device='cuda:0')}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21c689-5f90-4631-91e1-d3c92c9b414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "optimizer = optim.AdamP(m.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = ExponentialLR(optimizer, gamma=lr_gamma)\n",
    "loss_function = torch.functional.F.binary_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ec3950-4e65-4dcb-9d6d-6e792a825889",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(m, dl_test, loss_function, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e6ac53-c780-41bd-abb2-f0339475a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_ls = []\n",
    "test_loss_ls = []\n",
    "metrics_ls = []\n",
    "metrics_agg_ls = []\n",
    "\n",
    "# progress plot\n",
    "pp_metrics = ProgressPlot(x_lim=[1,num_epochs], y_lim=[0,1], plot_names = ['metrics'], x_label=\"Epoch\", line_names=['AUROC', 'Avg. Prec.', 'Acc.', 'Sensitivity', 'Specificity'])\n",
    "\n",
    "best_test_loss = 1000\n",
    "is_frozen = True\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"-\"*40)\n",
    "    print(f\"Epoch {epoch+1} of {num_epochs}:\")\n",
    "    \n",
    "    # maybe unfreeze \n",
    "    if epoch >= unfreeze_after_n and is_frozen:\n",
    "        print(\"Unfreezing model encoder.\")\n",
    "        is_frozen=False\n",
    "        for p in m.encoder.parameters():\n",
    "            p.requires_grad = True\n",
    "            \n",
    "        # set all learning rates to the lower lr_unfrozen learning rate\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr_unfrozen\n",
    "\n",
    "    # train for a single epoch\n",
    "    train_loss = train_one_epoch(m, dl_train, loss_function, device)\n",
    "    train_loss_ls.append(train_loss)\n",
    "    print(f\"Training:\")\n",
    "    print(f\"\\tcross_entropy = {train_loss:0.3f}\")       \n",
    "\n",
    "    # evaluate\n",
    "    test_loss, metrics = evaluate(m, dl_test, loss_function, device)\n",
    "    test_loss_ls.append(test_loss)\n",
    "    metrics_ls.append(metrics)\n",
    "    print(f\"Test:\")\n",
    "    print(f\"\\tcross_entropy = {test_loss:0.3f}\")\n",
    "    print(f\"\\tmetrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"\\t\\t{k} = {v:0.3f}\")\n",
    "\n",
    "    if test_loss < best_test_loss:\n",
    "        torch.save(m.state_dict(), f\"{artifact_folder}/model_checkpoint_video.ckpt\")\n",
    "        best_test_loss = test_loss\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "    # TODO: use study-aggregated metrics\n",
    "    pp_metrics.update([[v for _,v in metrics.items()]])\n",
    "\n",
    "pp_metrics.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee357be-c34d-4267-86d3-2fbab3060ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDA",
   "language": "python",
   "name": "pda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
