{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daaa341c-0948-46ae-8454-8e47c62cf06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db27084b-c3c0-4e7a-bc00-dab2262def34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from torch.utils.data import DataLoader\n",
    "import timm\n",
    "from timm import optim, scheduler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics as skmet\n",
    "from jupyterplot import ProgressPlot\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "\n",
    "import transforms as my_transforms\n",
    "%aimport dataset\n",
    "from models import MultiTaskFrameClassifier\n",
    "ImageData = dataset.ImageData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af05b89-c3b2-4d84-8f4a-ab907f26fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_folder = '/zfs/wficai/pda/model_run_artifacts/20220818_multitask_224x224'\n",
    "# artifact_folder = '/zfs/wficai/pda/model_run_artifacts/20220818_all_224x224'\n",
    "# artifact_folder = '/zfs/wficai/pda/model_run_artifacts/20220818_justcolor_224x224'\n",
    "os.makedirs(artifact_folder, exist_ok=True)\n",
    "\n",
    "datestamp = '20220901'\n",
    "\n",
    "# Note: all configurations are packaged as dict for easy saving\n",
    "cfg = dict(\n",
    "    sanity_check = False,\n",
    "    sanity_check_frac = 0.1,\n",
    "    mode_filter =  ['2d', 'color', 'color_compare'],\n",
    "    view_filter = ['pdaView', 'pdaRelatedView', 'nonPDAView'],\n",
    "    test_frac = 0.25,\n",
    "    bs_train = 256,  # batch size for training\n",
    "    bs_test = 500,  # batch size for testing\n",
    "    num_workers = 10,  # number of parallel data loading workers\n",
    "    res = 224, # pixel size along height and width\n",
    "    device = 'cuda:0',\n",
    "    model = 'resnet50d',\n",
    "    weights = {'type': 1.0, 'mode': 0.1, 'view': 0.1},\n",
    "    num_epochs=12,\n",
    "    lr = 0.001,\n",
    "    lr_gamma = 0.92,\n",
    "    dropout = 0.3,\n",
    "    weight_decay = 0.001,\n",
    "    pretrained=True,\n",
    "    unfreeze_after_n=2,\n",
    "    lr_unfrozen = 0.00001,\n",
    "    in_paths = dict(\n",
    "        frame = f'/zfs/wficai/pda/model_data/{datestamp}_frame.csv',\n",
    "        video = f'/zfs/wficai/pda/model_data/{datestamp}_video.csv',\n",
    "        study = f'/zfs/wficai/pda/model_data/{datestamp}_study.csv',\n",
    "        patient_study = f'/zfs/wficai/pda/model_data/{datestamp}_patient_study.csv',\n",
    "        patient = f'/zfs/wficai/pda/model_data/{datestamp}_patient.csv'\n",
    "    ),\n",
    "    out_paths = dict(\n",
    "        train = 'train.csv',\n",
    "        test = 'test.csv'\n",
    "    ),\n",
    "    transforms = dict(\n",
    "        train = 'train',\n",
    "        test = 'test'\n",
    "    )\n",
    ")\n",
    "\n",
    "with open(artifact_folder + '/config.json', 'w') as f: \n",
    "    json.dump(cfg, f, indent=4)\n",
    "    \n",
    "# put all config variables in scope to avoid the need to laboriously index cfg\n",
    "for k, v in cfg.items():\n",
    "    v = f\"'{v}'\" if type(v)==str else v\n",
    "    exec(f\"{k}={v}\")\n",
    "del cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40792ac2-3896-4333-be17-a0c151423d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bd114bb-8654-42da-875d-0bb19d74fb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frame = pd.read_csv(in_paths['frame'])\n",
    "df_video = pd.read_csv(in_paths['video'])\n",
    "df_study = pd.read_csv(in_paths['study'])\n",
    "df_patient_study = pd.read_csv(in_paths['patient_study'])\n",
    "df_patient = pd.read_csv(in_paths['patient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50e7023d-8c68-46a5-b3fc-b3e8072550ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nopda    76\n",
       "pda      45\n",
       "Name: patient_type, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_study.patient_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3c9d4cc-b931-4e85-bb12-ec828a9ebb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((181809, 14), (44961, 14))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_patient_train, df_patient_test = train_test_split(df_patient, test_size=test_frac, shuffle=True)\n",
    "df_train = df_patient_train.merge(df_patient_study).merge(df_study, on=['patient_type', 'study']).merge(df_video, on=['patient_type', 'study']).merge(df_frame, on=['patient_type', 'external_id'])\n",
    "df_test = df_patient_test.merge(df_patient_study).merge(df_study, on=['patient_type', 'study']).merge(df_video, on=['patient_type', 'study']).merge(df_frame, on=['patient_type', 'external_id'])\n",
    "\n",
    "\n",
    "df_train.to_csv(f\"{artifact_folder}/{out_paths['train']}\", index=False)\n",
    "df_test.to_csv(f\"{artifact_folder}/{out_paths['test']}\", index=False)\n",
    "\n",
    "if sanity_check: \n",
    "    df_train = df_train.sample(frac=sanity_check_frac)\n",
    "    df_test = df_test.sample(frac=sanity_check_frac)\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab5c0db-7213-43dd-b85f-53ad4596d8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All disjoint checks passed\n"
     ]
    }
   ],
   "source": [
    "# ensure that patients are disjoint\n",
    "train_patient = set(df_train.patient_id)\n",
    "test_patient = set(df_test.patient_id)\n",
    "assert train_patient.isdisjoint(test_patient), 'Set of train patients and set of test patients are not disjoint!'\n",
    "\n",
    "# ensure that studies are disjoint\n",
    "train_study = set(df_train.study + df_train.patient_type)\n",
    "test_study = set(df_test.study + df_test.patient_type)\n",
    "assert train_study.isdisjoint(test_study), 'Set of train studies and set of test studies are not disjoint!'\n",
    "\n",
    "# ensure that videos are disjoint\n",
    "train_vids = set(df_train.external_id + df_train.patient_type)\n",
    "test_vids = set(df_test.external_id + df_test.patient_type)\n",
    "assert train_vids.isdisjoint(test_vids), 'Set of train videos and set of test videos are not disjoint!'\n",
    "\n",
    "# ensure that frames are disjoint\n",
    "train_frames = set(df_train.png_path)\n",
    "test_frames = set(df_test.png_path)\n",
    "assert train_frames.isdisjoint(test_frames), 'Set of train frames and set of test frames are not disjoint!'\n",
    "\n",
    "print(\"All disjoint checks passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c46042a3-6018-4497-ae4a-a40317232777",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = my_transforms.ImageTransforms(res)\n",
    "tfms_train = tfms.get_transforms(transforms['train'])\n",
    "tfms_test = tfms.get_transforms(transforms['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f19561d1-c7f0-4131-bb27-2bb32f19141a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size after filtering: 181809\n",
      "Test data size after filtering: 44961\n"
     ]
    }
   ],
   "source": [
    "# create datasets\n",
    "d_train = ImageData(df_train, transforms = tfms_train, mode_filter = mode_filter, view_filter = view_filter)\n",
    "dl_train = DataLoader(d_train, batch_size=bs_train, num_workers=num_workers, shuffle=True)\n",
    "\n",
    "d_test = ImageData(df_test, transforms = tfms_test, mode_filter = mode_filter, view_filter = view_filter)\n",
    "dl_test = DataLoader(d_test, batch_size=bs_test, num_workers=num_workers)\n",
    "\n",
    "print(\"Train data size after filtering:\", len(d_train))\n",
    "print(\"Test data size after filtering:\", len(d_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4430e70-798a-413d-917c-9bce0caa89bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = next(iter(dl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2df49f61-e396-46e1-9af9-9fa212099aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_dataloader, loss_function, device):\n",
    "    model.train()\n",
    "\n",
    "    num_steps_per_epoch = len(train_dataloader)\n",
    "\n",
    "    losses = []\n",
    "    for ix, batch in enumerate(train_dataloader):\n",
    "        inputs = batch['img'].to(device)\n",
    "        targets = {k: batch[k].to(device).type(torch.float32) for k in ['trg_type', 'trg_mode', 'trg_view']}\n",
    "        \n",
    "        predictions = model(inputs)\n",
    "\n",
    "        loss = loss_function(predictions, targets, weights)\n",
    "        loss['total'].backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        losses.append({k: v.detach().item() for k, v in loss.items()})\n",
    "        print(f\"\\tBatch {ix+1} of {num_steps_per_epoch}. Loss={loss['total'].detach().item():0.3f}\", end='\\r')\n",
    "    \n",
    "    print(' '*100, end='\\r')\n",
    "        \n",
    "    losses = pd.DataFrame(losses).mean()\n",
    "    return losses\n",
    "            \n",
    "def evaluate(model, test_dataloader, loss_function, device):\n",
    "    model.eval()\n",
    "\n",
    "    num_steps_per_epoch = len(test_dataloader)\n",
    "\n",
    "    patient_ls = []\n",
    "    target_ls = []\n",
    "    output_ls = []\n",
    "    losses = []\n",
    "    for ix, batch in enumerate(test_dataloader):\n",
    "        inputs = batch['img'].to(device)\n",
    "        targets = {k: batch[k].to(device).type(torch.float32) for k in ['trg_type', 'trg_mode', 'trg_view']}\n",
    "        target_ls.append(targets)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = model(inputs)\n",
    "            output_ls.append(predictions)\n",
    "            loss = loss_function(predictions, targets, weights)\n",
    "            \n",
    "        losses.append(loss)\n",
    "        \n",
    "    #compute metrics\n",
    "    \n",
    "    metrics = compute_metrics(target_ls, output_ls)\n",
    "    \n",
    "    #average loss\n",
    "    avg_losses = pd.DataFrame(losses).mean()\n",
    "    \n",
    "    return avg_losses, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d0192d3-c53f-4e8e-bf7b-656a2270813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(target_ls, output_ls):\n",
    "    y_true = torch.concat([trg['trg_type'] for trg in target_ls]).detach().cpu().numpy()\n",
    "    y_pred = torch.concat([out['type'] for out in output_ls]).detach().cpu().numpy().squeeze()\n",
    "    \n",
    "    # filter out nonPDAViews and 2d images when computing type prediction metrics\n",
    "    trg_mode = torch.concat([trg['trg_mode'] for trg in target_ls]).detach().cpu().numpy()\n",
    "    trg_view = torch.concat([trg['trg_view'] for trg in target_ls]).detach().cpu().numpy()\n",
    "    type_filter = (trg_view==0) | (trg_mode==0)\n",
    "    y_true = y_true[~type_filter]\n",
    "    y_pred = y_pred[~type_filter]\n",
    "    \n",
    "    y_pred = 1/(1+np.exp(-y_pred))\n",
    "    y_pred_cls = (y_pred>0.5).astype(int)\n",
    "    \n",
    "    mets = dict()    \n",
    "    mets['roc_auc'] = skmet.roc_auc_score(y_true, y_pred)\n",
    "    mets['average_precision'] = skmet.average_precision_score(y_true, y_pred)\n",
    "    mets['accuracy'] = skmet.accuracy_score(y_true, y_pred_cls)\n",
    "    mets['sensitivity'] = skmet.recall_score(y_true, y_pred_cls)\n",
    "    mets['specificity'] = skmet.recall_score(y_true, y_pred_cls, pos_label=0)\n",
    "    \n",
    "    return mets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f59b10c2-a6ec-43c1-9b24-82e56d096ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "is_encoder_frozen = True if unfreeze_after_n>0 else False    \n",
    "encoder = timm.create_model(model, pretrained=pretrained, num_classes=1, in_chans=3, drop_rate=dropout)\n",
    "clf = MultiTaskFrameClassifier(encoder, encoder_frozen=is_encoder_frozen).to(device)\n",
    "loss_func = MultiTaskFrameClassifier.multi_task_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0411402-8561-4909-a4eb-da69f9fd4f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': tensor(0.4156, device='cuda:0', grad_fn=<MeanBackward1>),\n",
       " 'type': tensor(0.1845, device='cuda:0', grad_fn=<MeanBackward1>),\n",
       " 'type_filtered': tensor(0.7155, device='cuda:0', grad_fn=<MeanBackward1>),\n",
       " 'mode': tensor(1.1537, device='cuda:0', grad_fn=<MeanBackward1>),\n",
       " 'view': tensor(1.1581, device='cuda:0', grad_fn=<MeanBackward1>)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = clf(test_batch['img'].to(device))\n",
    "targets = {k: test_batch[k].to(device) for k in ['trg_type', 'trg_mode', 'trg_view']}\n",
    "loss_dict = loss_func(outputs, targets, weights=weights)\n",
    "loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6fb49fc-332d-440d-905e-e48a6593c670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(total            0.386604\n",
       " type             0.153986\n",
       " type_filtered         NaN\n",
       " mode             1.168871\n",
       " view             1.157308\n",
       " dtype: float64,\n",
       " {'roc_auc': 0.40852027752546866,\n",
       "  'average_precision': 0.44109200866136505,\n",
       "  'accuracy': 0.47419388070464613,\n",
       "  'sensitivity': 0.029085033326600687,\n",
       "  'specificity': 0.9375525651808242})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(clf, dl_test, loss_func, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43063e9b-b9ae-494a-91b2-3d0b3f280b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Epoch 1 of 12:\n",
      "Training:                                                                                           \n",
      "\tcross_entropy:\n",
      "\t\ttotal = 0.259\n",
      "\t\ttype = 0.147\n",
      "\t\ttype_filtered = 0.602\n",
      "\t\tmode = 0.336\n",
      "\t\tview = 0.785\n",
      "Test:\n",
      "\tcross_entropy:\n",
      "\t\ttotal = 0.202\n",
      "\t\ttype = 0.124\n",
      "\t\ttype_filtered = nan\n",
      "\t\tmode = 0.135\n",
      "\t\tview = 0.648\n",
      "\tmetrics (type):\n",
      "\t\troc_auc = 0.850\n",
      "\t\taverage_precision = 0.855\n",
      "\t\taccuracy = 0.687\n",
      "\t\tsensitivity = 0.961\n",
      "\t\tspecificity = 0.401\n",
      "----------------------------------------\n",
      "Epoch 2 of 12:\n",
      "Training:                                                                                           \n",
      "\tcross_entropy:\n",
      "\t\ttotal = 0.240\n",
      "\t\ttype = 0.141\n",
      "\t\ttype_filtered = 0.575\n",
      "\t\tmode = 0.236\n",
      "\t\tview = 0.755\n",
      "Test:\n",
      "\tcross_entropy:\n",
      "\t\ttotal = 0.204\n",
      "\t\ttype = 0.127\n",
      "\t\ttype_filtered = nan\n",
      "\t\tmode = 0.127\n",
      "\t\tview = 0.638\n",
      "\tmetrics (type):\n",
      "\t\troc_auc = 0.857\n",
      "\t\taverage_precision = 0.866\n",
      "\t\taccuracy = 0.663\n",
      "\t\tsensitivity = 0.969\n",
      "\t\tspecificity = 0.344\n",
      "----------------------------------------\n",
      "Epoch 3 of 12:\n",
      "Unfreezing model encoder.\n",
      "Training:                                                                                           \n",
      "\tcross_entropy:\n",
      "\t\ttotal = 0.220\n",
      "\t\ttype = 0.132\n",
      "\t\ttype_filtered = 0.538\n",
      "\t\tmode = 0.160\n",
      "\t\tview = 0.719\n",
      "Test:\n",
      "\tcross_entropy:\n",
      "\t\ttotal = 0.183\n",
      "\t\ttype = 0.115\n",
      "\t\ttype_filtered = nan\n",
      "\t\tmode = 0.096\n",
      "\t\tview = 0.591\n",
      "\tmetrics (type):\n",
      "\t\troc_auc = 0.879\n",
      "\t\taverage_precision = 0.900\n",
      "\t\taccuracy = 0.705\n",
      "\t\tsensitivity = 0.939\n",
      "\t\tspecificity = 0.462\n",
      "----------------------------------------\n",
      "Epoch 4 of 12:\n",
      "Training:                                                                                           \n",
      "\tcross_entropy:\n",
      "\t\ttotal = 0.196\n",
      "\t\ttype = 0.117\n",
      "\t\ttype_filtered = 0.477\n",
      "\t\tmode = 0.116\n",
      "\t\tview = 0.671\n",
      "Test:\n",
      "\tcross_entropy:\n",
      "\t\ttotal = 0.173\n",
      "\t\ttype = 0.107\n",
      "\t\ttype_filtered = nan\n",
      "\t\tmode = 0.098\n",
      "\t\tview = 0.560\n",
      "\tmetrics (type):\n",
      "\t\troc_auc = 0.889\n",
      "\t\taverage_precision = 0.911\n",
      "\t\taccuracy = 0.736\n",
      "\t\tsensitivity = 0.925\n",
      "\t\tspecificity = 0.539\n",
      "----------------------------------------\n",
      "Epoch 5 of 12:\n",
      "Training:                                                                                           \n",
      "\tcross_entropy:\n",
      "\t\ttotal = 0.180\n",
      "\t\ttype = 0.106\n",
      "\t\ttype_filtered = 0.433\n",
      "\t\tmode = 0.102\n",
      "\t\tview = 0.643\n",
      "Test:\n",
      "\tcross_entropy:\n",
      "\t\ttotal = 0.166\n",
      "\t\ttype = 0.102\n",
      "\t\ttype_filtered = nan\n",
      "\t\tmode = 0.099\n",
      "\t\tview = 0.543\n",
      "\tmetrics (type):\n",
      "\t\troc_auc = 0.888\n",
      "\t\taverage_precision = 0.911\n",
      "\t\taccuracy = 0.761\n",
      "\t\tsensitivity = 0.899\n",
      "\t\tspecificity = 0.616\n",
      "----------------------------------------\n",
      "Epoch 6 of 12:\n",
      "Training:                                                                                           \n",
      "\tcross_entropy:\n",
      "\t\ttotal = 0.167\n",
      "\t\ttype = 0.095\n",
      "\t\ttype_filtered = 0.388\n",
      "\t\tmode = 0.094\n",
      "\t\tview = 0.621\n",
      "Test:\n",
      "\tcross_entropy:\n",
      "\t\ttotal = 0.164\n",
      "\t\ttype = 0.101\n",
      "\t\ttype_filtered = nan\n",
      "\t\tmode = 0.099\n",
      "\t\tview = 0.529\n",
      "\tmetrics (type):\n",
      "\t\troc_auc = 0.883\n",
      "\t\taverage_precision = 0.906\n",
      "\t\taccuracy = 0.771\n",
      "\t\tsensitivity = 0.881\n",
      "\t\tspecificity = 0.656\n",
      "----------------------------------------\n",
      "Epoch 7 of 12:\n",
      "Training:                                                                                           \n",
      "\tcross_entropy:\n",
      "\t\ttotal = 0.155\n",
      "\t\ttype = 0.086\n",
      "\t\ttype_filtered = 0.351\n",
      "\t\tmode = 0.087\n",
      "\t\tview = 0.603\n",
      "Test:\n",
      "\tcross_entropy:\n",
      "\t\ttotal = 0.165\n",
      "\t\ttype = 0.103\n",
      "\t\ttype_filtered = nan\n",
      "\t\tmode = 0.099\n",
      "\t\tview = 0.518\n",
      "\tmetrics (type):\n",
      "\t\troc_auc = 0.881\n",
      "\t\taverage_precision = 0.902\n",
      "\t\taccuracy = 0.775\n",
      "\t\tsensitivity = 0.877\n",
      "\t\tspecificity = 0.669\n",
      "----------------------------------------\n",
      "Epoch 8 of 12:\n",
      "Training:                                                                                           \n",
      "\tcross_entropy:\n",
      "\t\ttotal = 0.145\n",
      "\t\ttype = 0.078\n",
      "\t\ttype_filtered = 0.317\n",
      "\t\tmode = 0.083\n",
      "\t\tview = 0.587\n",
      "Test:\n",
      "\tcross_entropy:\n",
      "\t\ttotal = 0.173\n",
      "\t\ttype = 0.113\n",
      "\t\ttype_filtered = nan\n",
      "\t\tmode = 0.099\n",
      "\t\tview = 0.506\n",
      "\tmetrics (type):\n",
      "\t\troc_auc = 0.882\n",
      "\t\taverage_precision = 0.900\n",
      "\t\taccuracy = 0.763\n",
      "\t\tsensitivity = 0.902\n",
      "\t\tspecificity = 0.619\n",
      "----------------------------------------\n",
      "Epoch 9 of 12:\n",
      "\tBatch 495 of 711. Loss=0.129\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         g[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m lr_unfrozen\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# train for a single epoch\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m train_loss_ls\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_dataloader, loss_function, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m targets \u001b[38;5;241m=\u001b[39m {k: batch[k]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrg_type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrg_mode\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrg_view\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     11\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m---> 13\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Code/pda_detection/code/models.py:75\u001b[0m, in \u001b[0;36mMultiTaskFrameClassifier.multi_task_loss\u001b[0;34m(outputs, targets, weights)\u001b[0m\n\u001b[1;32m     61\u001b[0m lview \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(\n\u001b[1;32m     62\u001b[0m     outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mview\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     63\u001b[0m     targets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrg_view\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mlong), \n\u001b[1;32m     64\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     65\u001b[0m )\n\u001b[1;32m     67\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m     68\u001b[0m     weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m ltype \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m     69\u001b[0m     weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m lmode \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m     70\u001b[0m     weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mview\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m lview\n\u001b[1;32m     72\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m'\u001b[39m: total_loss\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: ltype\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype_filtered\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mltype\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mtype_filter\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m: lmode\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mview\u001b[39m\u001b[38;5;124m'\u001b[39m: lview\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     78\u001b[0m }\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_dict\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit\n",
    "optimizer = optim.AdamP(clf.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = ExponentialLR(optimizer, gamma=lr_gamma)\n",
    "\n",
    "train_loss_ls = []\n",
    "test_loss_ls = []\n",
    "metrics_ls = []\n",
    "\n",
    "best_test_loss = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"-\"*40)\n",
    "    print(f\"Epoch {epoch+1} of {num_epochs}:\")\n",
    "\n",
    "    # maybe unfreeze \n",
    "    if epoch >= unfreeze_after_n and is_encoder_frozen:\n",
    "        print(\"Unfreezing model encoder.\")\n",
    "        is_encoder_frozen=False\n",
    "        for p in clf.encoder.parameters():\n",
    "            p.requires_grad = True\n",
    "            \n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr_unfrozen\n",
    "\n",
    "\n",
    "    # train for a single epoch\n",
    "    train_loss = train_one_epoch(clf, dl_train, loss_func, device)\n",
    "    train_loss_ls.append(train_loss)\n",
    "    print(f\"Training:\")\n",
    "    print(\"\\tcross_entropy:\")\n",
    "    for k, v in train_loss.items():\n",
    "          print(f\"\\t\\t{k} = {v:0.3f}\") \n",
    "\n",
    "    # evaluate\n",
    "    test_loss, metrics = evaluate(clf, dl_test, loss_func, device)\n",
    "    test_loss_ls.append(test_loss)\n",
    "    # metrics_ls.append(metrics)\n",
    "    print(f\"Test:\")\n",
    "    print(\"\\tcross_entropy:\")\n",
    "    for k, v in test_loss.items():\n",
    "          print(f\"\\t\\t{k} = {v:0.3f}\")\n",
    "    print(f\"\\tmetrics (type):\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"\\t\\t{k} = {v:0.3f}\")\n",
    "\n",
    "    # select models with the best type loss\n",
    "    if test_loss['type'] < best_test_loss:\n",
    "        torch.save(clf.state_dict(), f\"{artifact_folder}/model_checkpoint.ckpt\")\n",
    "        best_test_loss = test_loss['type']\n",
    "        \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deebf54a-5b6a-441f-a177-ca536219005d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDA",
   "language": "python",
   "name": "pda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
