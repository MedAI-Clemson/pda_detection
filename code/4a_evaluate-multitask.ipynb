{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daaa341c-0948-46ae-8454-8e47c62cf06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db27084b-c3c0-4e7a-bc00-dab2262def34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import timm\n",
    "from timm import optim, scheduler\n",
    "import torch\n",
    "from torchvision import transforms as tfm\n",
    "from sklearn import metrics as skmet\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import transforms as my_transforms\n",
    "\n",
    "%aimport dataset\n",
    "from models import MultiTaskFrameClassifier\n",
    "ImageData = dataset.ImageData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af05b89-c3b2-4d84-8f4a-ab907f26fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_folder = '/zfs/wficai/pda/model_run_artifacts/20220818_multitask_224x224'\n",
    "\n",
    "with open(artifact_folder + '/config.json', 'r') as f: \n",
    "    cfg = json.load(f)\n",
    "\n",
    "# put all config variables in scope to avoid the need to laboriously index cfg\n",
    "for k, v in cfg.items():\n",
    "    v = f\"'{v}'\" if type(v)==str else v\n",
    "    exec(f\"{k}={v}\")\n",
    "del cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a8168f5-8efb-4cb6-a39f-a31dc61f6414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally override settings\n",
    "view_filter = ['pdaView', 'pdaRelatedView', 'nonPDAView']\n",
    "mode_filter = ['2d', 'color', 'color_compare']\n",
    "device = torch.device('cuda:1')  # you may need 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c46042a3-6018-4497-ae4a-a40317232777",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = my_transforms.ImageTransforms(res)\n",
    "tfms_test = tfms.get_transforms(transforms['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bd114bb-8654-42da-875d-0bb19d74fb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames after filtering: 44961\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(f'{artifact_folder}/{out_paths[\"test\"]}')\n",
    "d_test = ImageData(df_test, transforms = tfms_test, mode_filter = mode_filter, view_filter = view_filter)\n",
    "dl_test = DataLoader(d_test, batch_size=bs_test, num_workers=num_workers)\n",
    "\n",
    "print(\"Number of frames after filtering:\", len(d_test.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30c043c1-975b-4d85-9c5a-a1eb7ccb545a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 90\r"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "encoder = timm.create_model(model, pretrained=pretrained, num_classes=1, in_chans=3, drop_rate=dropout)\n",
    "clf = MultiTaskFrameClassifier(encoder).to(device)    \n",
    "clf.load_state_dict(torch.load(f\"{artifact_folder}/model_checkpoint.ckpt\"))\n",
    "clf.eval()\n",
    "loss_function = MultiTaskFrameClassifier.multi_task_loss\n",
    "\n",
    "target_ls = []\n",
    "output_ls = []\n",
    "study_ls = []\n",
    "video_ls = []\n",
    "view_ls = []\n",
    "mode_ls = []\n",
    "losses = []\n",
    "\n",
    "for ix, batch in enumerate(dl_test):\n",
    "    print(f\"Batch {ix+1}\", end = \"\\r\")\n",
    "    inputs = batch['img'].to(device)\n",
    "    targets = {k: batch[k].to(device).type(torch.float32) for k in ['trg_type', 'trg_mode', 'trg_view']}\n",
    "    \n",
    "    target_ls.append(batch['trg_view'].numpy())\n",
    "    view_ls.append(batch['trg_view'].numpy())\n",
    "    mode_ls.append(batch['trg_mode'].numpy())\n",
    "    study_ls += batch['study']\n",
    "    video_ls += batch['video']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = clf(inputs)\n",
    "        output_ls.append(outputs)\n",
    "        loss = loss_function(outputs, targets, weights)\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693d176-8135-4240-9a0c-32e53e543752",
   "metadata": {},
   "source": [
    "# TODO: Pick up from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26e49e76-b38a-4f34-97b6-25c78b76d73c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-dimensional arrays cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m df_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m      2\u001b[0m     study \u001b[38;5;241m=\u001b[39m study_ls,\n\u001b[1;32m      3\u001b[0m     video \u001b[38;5;241m=\u001b[39m video_ls,\n\u001b[0;32m----> 4\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_ls\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(),\n\u001b[1;32m      5\u001b[0m     target \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(target_ls), \n\u001b[1;32m      6\u001b[0m     mode \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(mode_ls),\n\u001b[1;32m      7\u001b[0m     view \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(view_ls)\n\u001b[1;32m      8\u001b[0m ))\n\u001b[1;32m     10\u001b[0m df_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(ImageData\u001b[38;5;241m.\u001b[39minv_mode_map)\n\u001b[1;32m     11\u001b[0m df_results\u001b[38;5;241m.\u001b[39mview \u001b[38;5;241m=\u001b[39m df_results\u001b[38;5;241m.\u001b[39mview\u001b[38;5;241m.\u001b[39mmap(ImageData\u001b[38;5;241m.\u001b[39minv_view_map)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(dict(\n",
    "    study = study_ls,\n",
    "    video = video_ls,\n",
    "    predicted = np.concatenate(output_ls).squeeze(),\n",
    "    target = np.concatenate(target_ls), \n",
    "    mode = np.concatenate(mode_ls),\n",
    "    view = np.concatenate(view_ls)\n",
    "))\n",
    "\n",
    "df_results['mode'] = df_results['mode'].map(ImageData.inv_mode_map)\n",
    "df_results.view = df_results.view.map(ImageData.inv_view_map)\n",
    "\n",
    "df_results.predicted = 1 / (1 + np.exp(-df_results.predicted))\n",
    "\n",
    "df_results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0192d3-c53f-4e8e-bf7b-656a2270813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred, thresh=0.5):\n",
    "    mets = dict()\n",
    "    \n",
    "    y_pred_cls = (y_pred>thresh).astype(int)\n",
    "    \n",
    "    mets['num_samples'] = len(y_true)\n",
    "    mets['roc_auc'] = skmet.roc_auc_score(y_true, y_pred)\n",
    "    mets['average_precision'] = skmet.average_precision_score(y_true, y_pred)\n",
    "    mets['accuracy'] = skmet.accuracy_score(y_true, y_pred_cls)\n",
    "    mets['sensitivity'] = skmet.recall_score(y_true, y_pred_cls)\n",
    "    mets['specificity'] = skmet.recall_score(y_true, y_pred_cls, pos_label=0)\n",
    "    \n",
    "    return mets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b112b57-1b83-43f3-8fd3-1b24381cb1cd",
   "metadata": {},
   "source": [
    "# Frame-level results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0111d9a-2051-4af3-832d-b1a8702f2d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(df_results.target, df_results.predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd71340-60fd-4463-8e1c-98320b0eba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_results = df_results.groupby(['view', 'mode']).apply(lambda dat: compute_metrics(dat.target, dat.predicted))\n",
    "grouped_results = pd.DataFrame(grouped_results.tolist(), index=grouped_results.index)\n",
    "grouped_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343791d-605a-44a9-838e-2e165e8a4d34",
   "metadata": {},
   "source": [
    "# Clip-level results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fe47b7-64bf-45bd-a6a8-c17d40d32e63",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Avg confidence over frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca05df-6c84-435b-862b-0693e44f90e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_results_clip_avg = df_results.groupby(['study', 'video', 'target', 'view', 'mode'], as_index=False).agg('mean')\n",
    "display(df_results_clip_avg.head(10))\n",
    "\n",
    "compute_metrics(df_results_clip_avg.target, df_results_clip_avg.predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb200c7-418d-4cc8-ae1b-4f4e8c7dea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_results_clip_avg = df_results_clip_avg.\\\n",
    "    groupby(['view', 'mode']).\\\n",
    "    apply(lambda dat: compute_metrics(dat.target, dat.predicted, thresh=0.55))\n",
    "grouped_results_clip_avg = pd.DataFrame(grouped_results_clip_avg.tolist(), index=grouped_results.index)\n",
    "grouped_results_clip_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17612c45-2a36-46c0-a766-413c775eaa43",
   "metadata": {},
   "source": [
    "### Misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54044acf-8e37-40d2-a470-3abb8c19d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "true = df_results_clip_avg.target\n",
    "pred_cls = (df_results_clip_avg.predicted>0.5).astype(int)\n",
    "\n",
    "df_results_clip_avg[true != pred_cls]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bd834d-a793-475b-b41b-527bb031d133",
   "metadata": {},
   "source": [
    "# Study-level results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b172583-25f5-468b-a061-e42f055c81b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_study_avg = df_results_clip_avg.groupby(['study', 'target'], as_index=False).agg('mean')\n",
    "display(df_results_study_avg)\n",
    "\n",
    "compute_metrics(df_results_study_avg.target, df_results_study_avg.predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b87b2be-3b2a-441a-96ca-8f94ce18fb97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDA",
   "language": "python",
   "name": "pda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
